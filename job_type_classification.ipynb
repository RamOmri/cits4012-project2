{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20899,
     "status": "ok",
     "timestamp": 1653781727963,
     "user": {
      "displayName": "Omri Ram",
      "userId": "07837710711281572278"
     },
     "user_tz": -120
    },
    "id": "WGjnrVGyEhcU",
    "outputId": "c8ec1c01-b3a8-4325-9732-cc78d5b8ddfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "\n",
      "Your working directory was changed to /content/drive/My Drive/remote_proj\n",
      "\n",
      "An empty text file was created there. You can also run !pwd to confirm the current working directory.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# Set your working directory to a folder in your Google Drive. This way, if your notebook times out,\n",
    "# your files will be saved in your Google Drive!\n",
    "\n",
    "# the base Google Drive directory\n",
    "root_dir = \"/content/drive/My Drive/\"\n",
    "\n",
    "# choose where you want your project files to be saved\n",
    "project_folder = \"remote_proj\"\n",
    "\n",
    "def create_and_set_working_directory(project_folder):\n",
    "  # check if your project folder exists. if not, it will be created.\n",
    "  if os.path.isdir(root_dir + project_folder) == False:\n",
    "    os.mkdir(root_dir + project_folder)\n",
    "    print(root_dir + project_folder + ' did not exist but was created.')\n",
    "\n",
    "  # change the OS to use your project folder as the working directory\n",
    "  os.chdir(root_dir + project_folder)\n",
    "\n",
    "  # create a test file to make sure it shows up in the right place\n",
    "  !touch 'new_file_in_working_directory.txt'\n",
    "  print('\\nYour working directory was changed to ' + root_dir + project_folder + \\\n",
    "        \"\\n\\nAn empty text file was created there. You can also run !pwd to confirm the current working directory.\" )\n",
    "\n",
    "create_and_set_working_directory(project_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZAhFGMUEdRg"
   },
   "source": [
    "# Job Type classification with a feed forward neural netowrk and CNN\n",
    "22274048, 23191175"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 532,
     "status": "ok",
     "timestamp": 1653781795231,
     "user": {
      "displayName": "Omri Ram",
      "userId": "07837710711281572278"
     },
     "user_tz": -120
    },
    "id": "5PAcMgm5E156",
    "outputId": "8815a643-10e6-4199-829f-92c8acb7577c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 211,
     "status": "ok",
     "timestamp": 1653781800887,
     "user": {
      "displayName": "Omri Ram",
      "userId": "07837710711281572278"
     },
     "user_tz": -120
    },
    "id": "qairXn8IEdRl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "import string\n",
    "from argparse import Namespace\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import os\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from argparse import Namespace\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import Word2Vec\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "from nltk.corpus import stopwords\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import KeyedVectors\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 291,
     "status": "ok",
     "timestamp": 1653781801467,
     "user": {
      "displayName": "Omri Ram",
      "userId": "07837710711281572278"
     },
     "user_tz": -120
    },
    "id": "ZTcRWn2lEdRn",
    "outputId": "06d513db-cbfe-4395-f1cd-c850c3320cf4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\jcary\\\\Desktop\\\\cits4012-project2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "script_dir = os.path.dirname(os.path.realpath('__file__'))\n",
    "script_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1653781801467,
     "user": {
      "displayName": "Omri Ram",
      "userId": "07837710711281572278"
     },
     "user_tz": -120
    },
    "id": "oCFVVOouEdRp"
   },
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "     processed_csv = os.path.join(script_dir, 'data/processed_seek.csv'),\n",
    "     word2vec_model_filepath = os.path.join(script_dir, 'model_storage/word2vec'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3oOLqwDGEdRp"
   },
   "source": [
    "Load the csv file into a dataframe and the domain specific word2vec model and pretrained glove model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 90022,
     "status": "ok",
     "timestamp": 1653781891485,
     "user": {
      "displayName": "Omri Ram",
      "userId": "07837710711281572278"
     },
     "user_tz": -120
    },
    "id": "5C5Uh4tJEdRq",
    "outputId": "4170ede2-739c-4f2f-a1ae-33bba20110f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loaded object is of type <class 'gensim.models.keyedvectors.KeyedVectors'>\n"
     ]
    }
   ],
   "source": [
    "def load_embedding_model():\n",
    "    \"\"\" Load GloVe Vectors\n",
    "        Return:\n",
    "            wv_from_bin: All 400000 embeddings, each lengh 200\n",
    "    \"\"\"\n",
    "    import gensim.downloader as api\n",
    "    wv_pretrained = api.load(\"glove-wiki-gigaword-100\")\n",
    "    #print(\"Loaded vocab size %i\" % len(wv_pretrained.key_to_index))\n",
    "    print(\"The loaded object is of type %s\" % str(type(wv_pretrained)))\n",
    "    return wv_pretrained\n",
    "glove_model = load_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 3937,
     "status": "ok",
     "timestamp": 1653781895419,
     "user": {
      "displayName": "Omri Ram",
      "userId": "07837710711281572278"
     },
     "user_tz": -120
    },
    "id": "PfuYVTsPEdRq",
    "outputId": "8908fdf1-a548-45a3-f35f-4f5aa8083ff7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>job_description</th>\n",
       "      <th>job_type_target</th>\n",
       "      <th>short_description</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Retail &amp; Consumer Products</td>\n",
       "      <td>[have, you, had, years, experience, in, fresh,...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>[fresh, produce, food, fresh, food, fresh, foo...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Government &amp; Defence</td>\n",
       "      <td>[the, opportunity, the, client, solution, anal...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>[solution, analyst, requests, resolution, requ...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Trades &amp; Services</td>\n",
       "      <td>[an, innovative, business, development, role, ...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>[winning, van, house, joining, excellent, focu...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trades &amp; Services</td>\n",
       "      <td>[about, the, role, we, are, seeking, an, autom...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>[springvale, line, members, members, tmre, mec...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trades &amp; Services</td>\n",
       "      <td>[early, starts, and, weekend, shifts, no, expe...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>[early, starts, weekend, shifts, necessary, gr...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29650</th>\n",
       "      <td>Hospitality &amp; Tourism</td>\n",
       "      <td>[hotel, snapshot, the, radisson, blu, plaza, s...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>[hotel, star, hotel, located, located, bar, su...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29651</th>\n",
       "      <td>CEO &amp; General Management</td>\n",
       "      <td>[the, organisation, airservices, is, governmen...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>[airservices, aviation, air, navigation, airse...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29652</th>\n",
       "      <td>Accounting</td>\n",
       "      <td>[about, the, company, and, role, our, client, ...</td>\n",
       "      <td>Full Time</td>\n",
       "      <td>[clinic, lowered, reporting, audit, process, p...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29653</th>\n",
       "      <td>Government &amp; Defence</td>\n",
       "      <td>[long, term, contract, for, months, with, poss...</td>\n",
       "      <td>Other</td>\n",
       "      <td>[el, account, managers, aps, account, managers...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29654</th>\n",
       "      <td>Government &amp; Defence</td>\n",
       "      <td>[customer, service, representative, west, wyal...</td>\n",
       "      <td>Other</td>\n",
       "      <td>[customer, service, hours, hours, rotating, ro...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29655 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         category  \\\n",
       "0      Retail & Consumer Products   \n",
       "1            Government & Defence   \n",
       "2               Trades & Services   \n",
       "3               Trades & Services   \n",
       "4               Trades & Services   \n",
       "...                           ...   \n",
       "29650       Hospitality & Tourism   \n",
       "29651    CEO & General Management   \n",
       "29652                  Accounting   \n",
       "29653        Government & Defence   \n",
       "29654        Government & Defence   \n",
       "\n",
       "                                         job_description job_type_target  \\\n",
       "0      [have, you, had, years, experience, in, fresh,...       Full Time   \n",
       "1      [the, opportunity, the, client, solution, anal...       Full Time   \n",
       "2      [an, innovative, business, development, role, ...       Full Time   \n",
       "3      [about, the, role, we, are, seeking, an, autom...       Full Time   \n",
       "4      [early, starts, and, weekend, shifts, no, expe...       Full Time   \n",
       "...                                                  ...             ...   \n",
       "29650  [hotel, snapshot, the, radisson, blu, plaza, s...       Full Time   \n",
       "29651  [the, organisation, airservices, is, governmen...       Full Time   \n",
       "29652  [about, the, company, and, role, our, client, ...       Full Time   \n",
       "29653  [long, term, contract, for, months, with, poss...           Other   \n",
       "29654  [customer, service, representative, west, wyal...           Other   \n",
       "\n",
       "                                       short_description  split  \n",
       "0      [fresh, produce, food, fresh, food, fresh, foo...  train  \n",
       "1      [solution, analyst, requests, resolution, requ...  train  \n",
       "2      [winning, van, house, joining, excellent, focu...  train  \n",
       "3      [springvale, line, members, members, tmre, mec...  train  \n",
       "4      [early, starts, weekend, shifts, necessary, gr...  train  \n",
       "...                                                  ...    ...  \n",
       "29650  [hotel, star, hotel, located, located, bar, su...   test  \n",
       "29651  [airservices, aviation, air, navigation, airse...   test  \n",
       "29652  [clinic, lowered, reporting, audit, process, p...   test  \n",
       "29653  [el, account, managers, aps, account, managers...   test  \n",
       "29654  [customer, service, hours, hours, rotating, ro...   test  \n",
       "\n",
       "[29655 rows x 5 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model = Word2Vec.load(args.word2vec_model_filepath).wv\n",
    "df = pd.read_csv(args.processed_csv)\n",
    "df.short_description = df.short_description.apply(lambda x: x.split(' '))\n",
    "df.job_description = df.job_description.apply(lambda x: x.split(' '))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvAtOs3YEdRr"
   },
   "source": [
    "## Implementing some helper classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUoIBTtpEdRs"
   },
   "source": [
    "The below clases are used in the implementaiton of the models in the next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1653781895420,
     "user": {
      "displayName": "Omri Ram",
      "userId": "07837710711281572278"
     },
     "user_tz": -120
    },
    "id": "SICamGtMEdRs"
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract Vocabulary for mapping\"\"\"\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existingmap of tokens to indices\n",
    "            add_unk (bool): a flag that indicates whether to add the UNK token\n",
    "            unk_token (str): the UNK token to add into the Vocabulary\n",
    "        \"\"\"\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx: token \n",
    "                                for token, idx in self._token_to_idx.items()}\n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        self.unk_index = 1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "    def to_serializable(self):\n",
    "        \"\"\" returns a dictionary that can be serialized \"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx,\n",
    "                'add_unk': self._add_unk,\n",
    "                'unk_token': self._unk_token}\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\" instantiates the Vocabulary from a serialized dictionary \"\"\"\n",
    "        return cls(**contents)\n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "    def add_many(self, tokens):\n",
    "        \"\"\"Add a list of tokens into the Vocabulary\n",
    "        \n",
    "        Args:\n",
    "            tokens (list): a list of string tokens\n",
    "        Returns:\n",
    "            indices (list): a list of indices corresponding to the tokens\n",
    "        \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token\n",
    "        or the UNK index if token isn't present.\n",
    "        Args:\n",
    "            token (str): the token to look up\n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary)\n",
    "            for the UNK functionality\n",
    "        \"\"\"\n",
    "        if self._add_unk:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "        Args:\n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "        KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1653781895420,
     "user": {
      "displayName": "Omri Ram",
      "userId": "07837710711281572278"
     },
     "user_tz": -120
    },
    "id": "ritXuAJ2EdRu"
   },
   "outputs": [],
   "source": [
    "class One_hot_Vectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"\n",
    "    def __init__(self, description_vocab, target_vocab):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            description_vocab (Vocabulary): maps words to integers\n",
    "            target_vocab (Vocabulary): maps class labels to integers\n",
    "        \"\"\"\n",
    "        self.description_vocab = description_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "    def vectorize(self, description_tokenized):\n",
    "        \"\"\"Create a collapsed one hot vector for the job description\n",
    "        Args:\n",
    "            description_tokenized (list): the tokenized job description\n",
    "        Returns:\n",
    "            one_hot (np.ndarray): the collapsed onehot encoding\n",
    "        \"\"\"\n",
    "        one_hot = np.zeros(len(self.description_vocab), dtype=np.float32)\n",
    "        for token in description_tokenized:\n",
    "            if token not in string.punctuation:\n",
    "                one_hot[self.description_vocab.lookup_token(token)] = 1\n",
    "        return one_hot\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, description_df, target_cat = True, cutoff=25):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        Args:\n",
    "            description_df (pandas.DataFrame): the description dataset\n",
    "            cutoff (int): the parameter for frequency based filtering\n",
    "        Returns:\n",
    "            an instance of the descriptionVectorizer\n",
    "        \"\"\"\n",
    "        description_vocab = Vocabulary(add_unk=True)\n",
    "        target_vocab = Vocabulary(add_unk=False)\n",
    "        # adding category or Job_type\n",
    "        if target_cat:\n",
    "            for category in sorted(set(description_df.category)):\n",
    "                target_vocab.add_token(category)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            for job_type in  sorted(set(description_df.job_type_target)):\n",
    "                target_vocab.add_token(job_type)\n",
    "        # Add top words if count > provided count\n",
    "        word_counts = Counter()\n",
    "        for description in description_df.job_description:\n",
    "            for word in description:\n",
    "                if word not in string.punctuation:\n",
    "                    word_counts[word] += 1\n",
    "        for word, count in word_counts.items():\n",
    "            if count > cutoff:\n",
    "                description_vocab.add_token(word)\n",
    "        return cls(description_vocab, target_vocab)\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\"Intantiate a descriptionVectorizer from a serializable dictionary\n",
    "        Args:\n",
    "            contents (dict): the serializable dictionary\n",
    "        Returns:\n",
    "            an instance of the descriptionVectorizer class\n",
    "        \"\"\"\n",
    "        description_vocab = Vocabulary.from_serializable(contents['description_vocab'])\n",
    "        target_vocab = Vocabulary.from_serializable(contents['target_vocab'])\n",
    "        return cls(description_vocab=description_vocab, target_vocab=target_vocab)\n",
    "    def to_serializable(self):\n",
    "        \"\"\"Create the serializable dictionary for caching\n",
    "        Returns:\n",
    "            contents (dict): the serializable dictionary\n",
    "        \"\"\"\n",
    "        return {'description_vocab': self.description_vocab.to_serializable(),\n",
    "                'target_vocab': self.target_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1653781895420,
     "user": {
      "displayName": "Omri Ram",
      "userId": "07837710711281572278"
     },
     "user_tz": -120
    },
    "id": "VhZAPauhEdRu"
   },
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self._unk_token,\n",
    "                         'mask_token': self._mask_token,\n",
    "                         'begin_seq_token': self._begin_seq_token,\n",
    "                         'end_seq_token': self._end_seq_token})\n",
    "        return contents\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token \n",
    "          or the UNK index if token isn't present.\n",
    "        \n",
    "        Args:\n",
    "            token (str): the token to look up \n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
    "              for the UNK functionality \n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1653781895421,
     "user": {
      "displayName": "Omri Ram",
      "userId": "07837710711281572278"
     },
     "user_tz": -120
    },
    "id": "eihsNoYiEdRv"
   },
   "outputs": [],
   "source": [
    "class Vectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"    \n",
    "    def __init__(self, desc_vocab, target_vocab):\n",
    "        self.description_vocab = desc_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "    def vectorize(self, description, vector_length=-1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            description (list) : tokenized description \n",
    "            vector_length (int): an argument for forcing the length of index vector\n",
    "        Returns:\n",
    "            the vetorized description (numpy.array)\n",
    "        \"\"\"\n",
    "        indices = []\n",
    "        \n",
    "        \n",
    "        indices.extend(self.description_vocab.lookup_token(token) \n",
    "                       for token in description)\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "        \n",
    "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "     \n",
    "        out_vector[:len(indices)] = indices\n",
    "       \n",
    "\n",
    "        return out_vector\n",
    "      \n",
    "    def vectorize_one_hot(self, description):\n",
    "        \"\"\"Create a collapsed one hot vector for the job description\n",
    "        Args:\n",
    "            description_tokenized (list): the tokenized job description\n",
    "        Returns:\n",
    "            one_hot (np.ndarray): the collapsed onehot encoding\n",
    "        \"\"\"\n",
    "        one_hot = np.zeros(len(self.description_vocab), dtype=np.float32)\n",
    "        for token in description:\n",
    "            if token not in string.punctuation:\n",
    "                one_hot[self.description_vocab.lookup_token(token)] = 1\n",
    "        return one_hot\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, df, feature_column = 'short_description', target_cat = False, cutoff=10):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "        Args:\n",
    "            description_df (pandas.DataFrame): the target dataset\n",
    "            cutoff (int): frequency threshold for including in Vocabulary \n",
    "        Returns:\n",
    "            an instance of the descriptionVectorizer\n",
    "        \"\"\"\n",
    "        target_vocab = Vocabulary(add_unk=False)\n",
    "        if target_cat:\n",
    "            for target in sorted(set(df.category)):\n",
    "                target_vocab.add_token(target)    \n",
    "        else:\n",
    "            for job_type in sorted(set(df.job_type_target)):\n",
    "                target_vocab.add_token(job_type)\n",
    "        \n",
    " \n",
    "        word_counts = Counter()\n",
    "        for desc in df[feature_column]:\n",
    "            desc = desc.split(' ') if type(desc) is not list else desc\n",
    "            for token in desc:\n",
    "                word_counts[token] += 1\n",
    "\n",
    "        desc_vocab = SequenceVocabulary()\n",
    "        for word, word_count in word_counts.items():\n",
    "            if word_count >= cutoff:\n",
    "                desc_vocab.add_token(word)\n",
    "        \n",
    "        return cls(desc_vocab, target_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        description_vocab = \\\n",
    "            SequenceVocabulary.from_serializable(contents['description_vocab'])\n",
    "        target_vocab =  \\\n",
    "            Vocabulary.from_serializable(contents['target_vocab'])\n",
    "\n",
    "        return cls(description_vocab=description_vocab, target_vocab=target_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'description_vocab': self.description_vocab.to_serializable(),\n",
    "                'target_vocab': self.target_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 340,
     "status": "ok",
     "timestamp": 1653781895757,
     "user": {
      "displayName": "Omri Ram",
      "userId": "07837710711281572278"
     },
     "user_tz": -120
    },
    "id": "_QGgLv79EdRw"
   },
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, df, vectorizer, short = True, job_type = True, one_hot = False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pandas.DataFrame): the dataset\n",
    "            vectorizer (GloveVectorizer): vectorizer instatiated from dataset\n",
    "        \"\"\"\n",
    "        self.one_hot = one_hot\n",
    "        self.df = df\n",
    "        self._vectorizer = vectorizer\n",
    "        self.short = short\n",
    "        self.job_type = job_type\n",
    "\n",
    "        # +1 if only using begin_seq, +2 if using both begin and end seq tokens\n",
    "        measure_len = lambda context: len(context)\n",
    "        if self.short:\n",
    "            self._max_seq_length = max(map(measure_len, df.short_description)) + 2\n",
    "        else:\n",
    "            self._max_seq_length = max(map(measure_len, df.job_description)) + 2\n",
    "        \n",
    "        self.train_df = self.df[self.df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.df[self.df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.df[self.df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "        # Class weights\n",
    "        if self.job_type:\n",
    "            class_counts = df.job_type_target.value_counts().to_dict()\n",
    "        else:\n",
    "            class_counts = df.category.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.target_vocab.lookup_token(item[0])\n",
    "        \n",
    "\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "        \n",
    "        \n",
    "    @classmethod\n",
    "    def load_df_and_make_vectorizer(cls, df, short = True, one_hot = False):\n",
    "        \"\"\"Load dataset and make a  vectorizer from scratch\n",
    "        \n",
    "        Args:\n",
    "            csv (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of Dataset\n",
    "        \"\"\"\n",
    "        train_df = df[df.split=='train']\n",
    "        return cls(df, Vectorizer.from_dataframe(train_df), short = short, one_hot = one_hot)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\"a static method for loading the vectorizer from file\n",
    "        \n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location of the serialized vectorizer\n",
    "        Returns:\n",
    "            an instance of GloveVectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return Vectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        \"\"\"saves the vectorizer to disk using json\n",
    "        \n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location to save the vectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "        \n",
    "        Args:\n",
    "            index (int): the index to the data point \n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "        if self.one_hot:\n",
    "            if self.short:\n",
    "                description_vector = \\\n",
    "                    self._vectorizer.vectorize_one_hot(row.short_description)\n",
    "            else:\n",
    "                description_vector = \\\n",
    "                    self._vectorizer.vectorize_one_hot(row.job_description)\n",
    "\n",
    "        else:\n",
    "            if self.short:\n",
    "                description_vector = \\\n",
    "                    self._vectorizer.vectorize(row.short_description, self._max_seq_length)\n",
    "            else:\n",
    "                description_vector = \\\n",
    "                    self._vectorizer.vectorize(row.job_description, self._max_seq_length)\n",
    "\n",
    "        if self.job_type:\n",
    "            target_index = \\\n",
    "                self._vectorizer.target_vocab.lookup_token(row.job_type_target)\n",
    "        else:\n",
    "            target_index = \\\n",
    "                self._vectorizer.target_vocab.lookup_token(row.category)\n",
    "\n",
    "\n",
    "        return {'x_data': description_vector,\n",
    "                'y_target': target_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "        \n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\" \n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjE3Sd4TEdRx"
   },
   "source": [
    "### Defining some helper functions for training each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix_of_vectors(wv_pretrained, required_words):\n",
    "    \"\"\" Put the GloVe vectors into a matrix M.\n",
    "        Param:\n",
    "            wv_pretrained: KeyedVectors object; the 400000 GloVe vectors loaded from file\n",
    "        Return:\n",
    "            M: numpy matrix shape (num words, 200) containing the vectors\n",
    "            word2Ind: dictionary mapping each word to its row number in M\n",
    "    \"\"\"\n",
    "    import random\n",
    "    words = list(wv_pretrained.key_to_index.keys())\n",
    "    print(\"Shuffling words ...\")\n",
    "    random.seed(224)\n",
    "    random.shuffle(words)\n",
    "    words = words[:10000]\n",
    "    print(\"Putting %i words into word2Ind and matrix M...\" % len(words))\n",
    "    word2Ind = {}\n",
    "    M = []\n",
    "    curInd = 0\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(wv_pretrained.get_vector(w, norm=True))\n",
    "            word2Ind[w] = curInd\n",
    "            curInd += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    for w in required_words:\n",
    "        if w in words:\n",
    "            continue\n",
    "        try:\n",
    "            M.append(wv_pretrained.get_vector(w, norm=True))\n",
    "            word2Ind[w] = curInd\n",
    "            curInd += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    M = np.stack(M)\n",
    "    print(\"Done.\")\n",
    "    return M, word2Ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1653781895758,
     "user": {
      "displayName": "Omri Ram",
      "userId": "07837710711281572278"
     },
     "user_tz": -120
    },
    "id": "LZo7gdZrEdRx"
   },
   "outputs": [],
   "source": [
    "def make_one_hot_matrix(one_hot_vectorizer, words):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "        one_hot_vectorizer: of class one_hot_vectorizer \n",
    "        words: List of words in the dataset\n",
    "    Returns:\n",
    "        A matrix of the embeddings for each word matching the word's index\n",
    "    \"\"\"\n",
    "    embedding_size = len(words)\n",
    "    final_embeddings = np.zeros((len(words), len(words)))\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "      final_embeddings[i, :] = one_hot_vectorizer.vectorize(word)\n",
    "    return final_embeddings\n",
    "    \n",
    "def make_embedding_matrix(embeddings, word_to_idx, words):\n",
    "    \"\"\"\n",
    "    Create embedding matrix for a specific set of words.\n",
    "    \n",
    "    Args:\n",
    "        glove_filepath (str): file path to the glove embeddigns\n",
    "        words (list): list of words in the dataseta\n",
    "    \"\"\"\n",
    "    embedding_size = embeddings.shape[1]\n",
    "    \n",
    "    final_embeddings = np.zeros((len(words), embedding_size))\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        if word in word_to_idx:\n",
    "            final_embeddings[i, :] = embeddings[word_to_idx[word]]\n",
    "        else:\n",
    "            embedding_i = torch.ones(1, embedding_size)\n",
    "            torch.nn.init.xavier_uniform_(embedding_i)\n",
    "            final_embeddings[i, :] = embedding_i\n",
    "\n",
    "    return final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1653781895758,
     "user": {
      "displayName": "Omri Ram",
      "userId": "07837710711281572278"
     },
     "user_tz": -120
    },
    "id": "JbcyGuMXEdRx"
   },
   "outputs": [],
   "source": [
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"): \n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will \n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict\n",
    "\n",
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 4,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"Handle the training state updates.\n",
    "\n",
    "    Components:\n",
    "     - Early Stopping: Prevent overfitting.\n",
    "     - Model Checkpoint: Model is saved if the model is better\n",
    "\n",
    "    :param args: main arguments\n",
    "    :param model: model to train\n",
    "    :param train_state: a dictionary representing the training state values\n",
    "    :returns:\n",
    "        a new train_state\n",
    "    \"\"\"\n",
    "\n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # If loss worsened\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu().long().to(cnn_args.device) #.max(dim=1)[1]\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n",
    "    \n",
    "def compute_accuracy_BCE(y_pred, y_target):\n",
    "    y_target = y_target.cpu()\n",
    "    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu().long()#.max(dim=1)[1]\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 267,
     "status": "ok",
     "timestamp": 1653781896022,
     "user": {
      "displayName": "Omri Ram",
      "userId": "07837710711281572278"
     },
     "user_tz": -120
    },
    "id": "kAHO3xBlEdRy"
   },
   "outputs": [],
   "source": [
    "def train_model(classifier, dataset, args,loss_func, BCE = False, one_hot = False):\n",
    "    \"\"\"\n",
    "    args:\n",
    "    classifier: the pytorch model that will be trained.\n",
    "    dataset: An object from the dataset class\n",
    "    returns:\n",
    "    the classifier and the final trainstate\n",
    "    \"\"\"\n",
    "    classifier = classifier.to(args.device)\n",
    "    dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "    \n",
    "    \n",
    "    optimizer = optim.Adam(classifier.parameters(), lr = args.learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, mode = 'min', factor=0.5, patience=1)\n",
    "\n",
    "    train_state = make_train_state(args)\n",
    "\n",
    "    epoch_bar = tqdm(desc='training routine', \n",
    "                            total=args.num_epochs,\n",
    "                            position=0)\n",
    "\n",
    "    dataset.set_split('train')\n",
    "    train_bar = tqdm(desc='split=train',\n",
    "                            total=dataset.get_num_batches(args.batch_size), \n",
    "                            position=1, \n",
    "                            leave=True)\n",
    "    dataset.set_split('val')\n",
    "    val_bar = tqdm(desc='split=val',\n",
    "                          total=dataset.get_num_batches(args.batch_size), \n",
    "                          position=1, \n",
    "                          leave=True)\n",
    "\n",
    "\n",
    "    classifier.float() # sets classifier to receive in double precision format\n",
    "    try:\n",
    "        \n",
    "        for epoch in range(args.num_epochs):\n",
    "            train_state['epoch_index'] = epoch\n",
    "            dataset.set_split('train')\n",
    "            batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            classifier.train()\n",
    "\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "                optimizer.zero_grad()\n",
    "                if BCE and one_hot:\n",
    "                    \n",
    "                    y_pred = classifier(batch_dict['x_data'].float())\n",
    "                    loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "                elif BCE and not one_hot:\n",
    "                    y_pred = classifier(batch_dict['x_data'])\n",
    "                    loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "                else:\n",
    "                    y_pred = classifier(batch_dict['x_data'])\n",
    "                    loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "                loss_t = loss.item()\n",
    "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                if BCE:\n",
    "                    acc_t = compute_accuracy_BCE(y_pred, batch_dict['y_target'])\n",
    "                else:\n",
    "                    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "                running_acc += (acc_t  - running_acc)/(batch_index+1)\n",
    "                #update bar\n",
    "                train_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                                      epoch=epoch)\n",
    "                train_bar.update()\n",
    "\n",
    "            train_state['train_loss'].append(running_loss)\n",
    "            train_state['train_acc'].append(running_acc)\n",
    "\n",
    "            dataset.set_split('val')\n",
    "            batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
    "            running_loss = 0.\n",
    "            running_acc = 0.\n",
    "            classifier.eval()\n",
    "\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "                \n",
    "                # step 3. compute the loss\n",
    "                if BCE and one_hot:\n",
    "                    \n",
    "                    y_pred = classifier(batch_dict['x_data'].float())\n",
    "                    loss = loss_func(y_pred.float(), batch_dict['y_target'].float())\n",
    "                elif BCE and not one_hot:\n",
    "                    y_pred = classifier(batch_dict['x_data'])\n",
    "                    loss = loss_func(y_pred.float(), batch_dict['y_target'].float())\n",
    "                else:\n",
    "                    y_pred = classifier(batch_dict['x_data'])\n",
    "                    loss = loss_func(y_pred.float(), batch_dict['y_target'].float())\n",
    "               \n",
    "                loss_t = loss.item()\n",
    "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "                if BCE:\n",
    "                     acc_t = compute_accuracy_BCE(y_pred, batch_dict['y_target'])\n",
    "                else:\n",
    "                    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "                val_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                                  epoch=epoch)\n",
    "                val_bar.update()\n",
    "            train_state['val_loss'].append(running_loss)\n",
    "            train_state['val_acc'].append(running_acc)\n",
    "\n",
    "            train_state = update_train_state(args=args, model=classifier,\n",
    "                                          train_state=train_state)\n",
    "\n",
    "            scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "            if train_state['stop_early']:\n",
    "                break\n",
    "            train_bar.n = 0\n",
    "            val_bar.n = 0\n",
    "            epoch_bar.update()\n",
    "\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Exiting loop\")\n",
    "    return classifier, train_state\n",
    "\n",
    "def test_model( classifier, dataset, train_state, args,\n",
    "               loss_func, BCE=False, one_hot = False):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        classifier to evaluate,\n",
    "        dataset: test data for evaluating the classifier,\n",
    "        train_state: the final train state after training the model,\n",
    "        BCE: whether to use binary cross entrop or cross entropy,\n",
    "    returns:\n",
    "        prints the classifier accuracy and loss\n",
    "    \"\"\"\n",
    "    dataset.set_split('test')\n",
    "    batch_generator = generate_batches(dataset, \n",
    "                                       batch_size=args.batch_size, \n",
    "                                       device=args.device)\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "    classifier.eval()\n",
    "\n",
    "    for batch_index, batch_dict in enumerate(batch_generator):\n",
    "        # compute the output\n",
    "        if BCE and one_hot:\n",
    "                    \n",
    "            y_pred = classifier(batch_dict['x_data'].float())\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "        elif BCE and not one_hot:\n",
    "            y_pred = classifier(batch_dict['x_data'])\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
    "        else:\n",
    "            y_pred = classifier(batch_dict['x_data'])\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "               \n",
    "        loss_t = loss.item()\n",
    "        running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "        if BCE:\n",
    "            acc_t = compute_accuracy_BCE(y_pred, batch_dict['y_target'])\n",
    "        else:\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "        running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "    train_state['test_loss'] = running_loss\n",
    "    train_state['test_acc'] = running_acc\n",
    "    print(\"Test loss: {};\".format(train_state['test_loss']))\n",
    "    print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ii76IthIEdRz"
   },
   "source": [
    "#### Initializing the dataset and deriving the vectorizer for each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NxkUbN3kEdRz"
   },
   "source": [
    "The dataset class can be fed into the dataloader that feeds the data as batches while training the classifier. The vectorizer matches all the tokens to indices which can be used when deriving the embeddings for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 626,
     "status": "ok",
     "timestamp": 1653781896647,
     "user": {
      "displayName": "Omri Ram",
      "userId": "07837710711281572278"
     },
     "user_tz": -120
    },
    "id": "7zquXmmiEdRz"
   },
   "outputs": [],
   "source": [
    "dataset_short_desc = Dataset.load_df_and_make_vectorizer(df)\n",
    "vectorizer_short_desc = dataset_short_desc.get_vectorizer()\n",
    "dataset_long_desc = Dataset.load_df_and_make_vectorizer(df, short=False)\n",
    "vectorizer_long_desc = dataset_long_desc.get_vectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying job type using a Feed forward binary classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to explore the effectiveness of the three vectorization methods employed in this assignment, the cnn model and a feed forward model will be trained using each embedding.Initially a short description with the top ten tfidf values will be used for each embedding before using the whole description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "# Defining arguments to be used for the feed forward neural network\n",
    "feed_forward_args = Namespace(\n",
    "    # Data and Path hyper parameters\n",
    "    \n",
    "    # Model hyper parameters\n",
    "    use_w2v = False,\n",
    "    use_glove=False, \n",
    "    hidden_dim=100, \n",
    "    input_size = None, # Since we are using top ten tfidf\n",
    "    vocab_size = None,\n",
    "    embedding_dim = 100,\n",
    "    target_dim = 2, # only need Full time and other.\n",
    "    # Training hyper parameter\n",
    "    seed=1337, \n",
    "    learning_rate=0.001, \n",
    "    batch_size=128,\n",
    "    dropout_p=0.1, \n",
    "    num_epochs=100, \n",
    "    early_stopping_criteria=5, \n",
    "    # Runtime option\n",
    "    cuda=True, \n",
    "    catch_keyboard_interrupt=True, \n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir='./model_storage/document_classification/',\n",
    "    device='cuda'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Classifier_feed_forward(nn.Module, ):\n",
    "    \"\"\" a simple perceptron-based classifier \"\"\"\n",
    "    def __init__(self, num_features = None, embedding_size= None,\n",
    "                 num_embeddings= None, hidden_dim= None,  \n",
    "                 input_size= None, dropout_p= None,\n",
    "                 pretrained_embeddings=None, one_hot = False):\n",
    "        self.one_hot = one_hot\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_features (int): the size of the input feature vector\n",
    "        \"\"\"\n",
    "        super(Classifier_feed_forward, self).__init__()\n",
    "        if self.one_hot:\n",
    "            \n",
    "            self.fc1 = nn.Linear(in_features=num_features, out_features=1)\n",
    "        else:\n",
    "            \n",
    "\n",
    "            if pretrained_embeddings is None:\n",
    "\n",
    "                self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
    "                                        num_embeddings=num_embeddings\n",
    "                                        )        \n",
    "            else:\n",
    "                pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
    "                self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
    "                                        num_embeddings=num_embeddings,\n",
    "                                        _weight=pretrained_embeddings)\n",
    "\n",
    "            self._dropout_p = dropout_p    \n",
    "\n",
    "            self.fc1 = nn.Linear(\n",
    "               input_size * embedding_size, \n",
    "                out_features=1\n",
    "            )\n",
    "            \n",
    "    def forward(self, x_in, apply_sigmoid=False):\n",
    "        \"\"\"The forward pass of the classifier\n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor \n",
    "                    x_in.shape should be (batch, num_features)\n",
    "            apply_sigmoid (bool): a flag for the sigmoid activation\n",
    "                    should be false if used with the cross-entropy losses\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch,).\n",
    "        \"\"\"\n",
    "        if self.one_hot:\n",
    "            y_out = self.fc1(x_in).squeeze()\n",
    "\n",
    "            if apply_sigmoid:\n",
    "                y_out = torch.sigmoid(y_out)\n",
    "            \n",
    "        else:\n",
    "            # mlp classifier\n",
    "       \n",
    "\n",
    "            x_embedded = self.emb(x_in).view(x_in.size()[0], -1)\n",
    "\n",
    "            y_out = self.fc1(x_embedded).squeeze()\n",
    "            if apply_sigmoid:\n",
    "                y_out = torch.sigmoid(y_out)\n",
    "\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneHot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_short_desc_one_hot = Dataset.load_df_and_make_vectorizer(df, one_hot= True)\n",
    "vectorizer = dataset_short_desc_one_hot.get_vectorizer()\n",
    "# model\n",
    "one_hot_classifier = Classifier_feed_forward(num_features=len(vectorizer.description_vocab), one_hot=True)\n",
    "feed_forward_args.save_dir ='./model_storage/feed_forward/one_hot_feed_forward.pth'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34c7e29c56e4e34ba64eaa8ae01bd04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e52b255f854fb483bce7d8fe84763f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af52cf8a49214c4ba7ccd908ddb6e5a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "one_hot_classifier, one_hot_train_state_ff = train_model(classifier=one_hot_classifier,\n",
    "                                                      dataset=dataset_short_desc_one_hot , args = feed_forward_args,\n",
    "                                                     loss_func=nn.BCEWithLogitsLoss(), BCE = True, one_hot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(feed_forward_args.save_dir, \"wb\") as outp:\n",
    "    pickle.dump(one_hot_classifier, outp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.5326642867015755;\n",
      "Test Accuracy: 73.57336956521736\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(feed_forward_args.save_dir, \"rb\") as outp:\n",
    "    one_hot_classifier = pickle.load( outp)\n",
    "\n",
    "test_model( one_hot_classifier, dataset_short_desc_one_hot,\n",
    "            one_hot_train_state_ff, feed_forward_args,\n",
    "          nn.BCEWithLogitsLoss(), BCE=True, one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### glove model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = dataset_short_desc.get_vectorizer()\n",
    "words = vectorizer.description_vocab._token_to_idx.keys()\n",
    "word_to_idx = {word: idx for idx, word in enumerate(glove_model.index_to_key)}\n",
    "\n",
    "glove_embeddings = make_embedding_matrix(glove_model.vectors, word_to_idx, words = words)\n",
    "feed_forward_args.input_size = dataset_short_desc._max_seq_length\n",
    "feed_forward_args.vocab_size = glove_embeddings.shape[0]\n",
    "feed_forward_args.embedding_dim = glove_embeddings.shape[1]\n",
    "feed_forward_args.embedding_dim = glove_embeddings.shape[1]\n",
    "feed_forward_args.save_dir ='./model_storage/feed_forward/glove_feed_forwar.pth'\n",
    "# model\n",
    "ff_classifier_short_glove = Classifier_feed_forward(embedding_size=feed_forward_args.embedding_dim, \n",
    "                            num_embeddings=len(vectorizer.description_vocab),\n",
    "                            hidden_dim=feed_forward_args.hidden_dim, \n",
    "                            input_size = feed_forward_args.input_size,\n",
    "                            dropout_p=feed_forward_args.dropout_p,\n",
    "                            pretrained_embeddings=glove_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a408ec066bb84f9089188459169eb22f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a941c25928642779f996c1f98b91e40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ef95bbf1544f568d1d70c80c2c458f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ff_classifier_short_glove, glove_train_state_ff = train_model(classifier=ff_classifier_short_glove,\n",
    "                                                      dataset=dataset_short_desc, args = feed_forward_args,\n",
    "                                                     loss_func=nn.BCEWithLogitsLoss(), BCE = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(feed_forward_args.save_dir, \"wb\") as outp:\n",
    "    pickle.dump(ff_classifier_short_glove, outp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(feed_forward_args.save_dir, \"rb\") as outp:\n",
    "    ff_classifier_short_glove = pickle.load( outp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6153544174588247;\n",
      "Test Accuracy: 72.91100543478264\n"
     ]
    }
   ],
   "source": [
    "test_model( ff_classifier_short_glove, dataset_short_desc,\n",
    "            glove_train_state_ff, feed_forward_args,\n",
    "          nn.BCEWithLogitsLoss(), BCE= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling words ...\n",
      "Putting 10000 words into word2Ind and matrix M...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "w2v_M, w2v_key_to_index = get_matrix_of_vectors(word2vec_model, words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_embeddings = make_embedding_matrix(w2v_M,\n",
    "                                            w2v_key_to_index, words = words)\n",
    "feed_forward_args.input_size = dataset_short_desc._max_seq_length\n",
    "feed_forward_args.vocab_size = word2vec_embeddings.shape[0]\n",
    "feed_forward_args.embedding_dim = word2vec_embeddings.shape[1]\n",
    "feed_forward_args.save_dir ='./model_storage/feed_forward/word2vec_feed_forward.pth'\n",
    "# model\n",
    "ff_classifier_short_word2vec = Classifier_feed_forward(embedding_size=feed_forward_args.embedding_dim, \n",
    "                            num_embeddings=len(vectorizer.description_vocab),\n",
    "                            hidden_dim=feed_forward_args.hidden_dim, \n",
    "                            input_size = feed_forward_args.input_size,\n",
    "                            dropout_p=feed_forward_args.dropout_p,\n",
    "                            pretrained_embeddings=word2vec_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8fc7c90bf164b3cb6949e62b9f1b74b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860fbf63d45948d7b031f5d60940f49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/162 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef3b662dc214b99b3c67b1aae1b61b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ff_classifier_short_word2vec, word2vec_train_state = train_model(classifier=ff_classifier_short_word2vec,\n",
    "                                                      dataset=dataset_short_desc, args = feed_forward_args,\n",
    "                                                     loss_func=nn.BCEWithLogitsLoss(), BCE = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(feed_forward_args.save_dir, \"wb\") as outp:\n",
    "    pickle.dump(ff_classifier_short_word2vec, outp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(feed_forward_args.save_dir, \"rb\") as outp:\n",
    "    ff_classifier_short_word2vec = pickle.load( outp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.6345301225133566;\n",
      "Test Accuracy: 74.03192934782608\n"
     ]
    }
   ],
   "source": [
    "test_model( ff_classifier_short_word2vec, dataset_short_desc,\n",
    "            glove_train_state, feed_forward_args,\n",
    "          nn.BCEWithLogitsLoss(), BCE= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-u-D4JrEdRz"
   },
   "source": [
    "## Classifying job type using a CNN binary classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ippECUt7EdR0"
   },
   "source": [
    "Here some arguments are defined to be used while training the cnn model. The cnn architecture was borrowed from lab 9 and was repurposed for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1653781896647,
     "user": {
      "displayName": "Omri Ram",
      "userId": "07837710711281572278"
     },
     "user_tz": -120
    },
    "id": "8qvhyIrvEdR0",
    "outputId": "5fbd7c45-ab1f-4597-af52-5e99f69c9de4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: False\n"
     ]
    }
   ],
   "source": [
    "cnn_args = Namespace(\n",
    "    num_epochs=100, \n",
    "    early_stopping_criteria=2, # The model tends to overfit very quickly\n",
    "    learning_rate=0.001, \n",
    "    dropout_p=0.5, \n",
    "    batch_size=50, \n",
    "    cuda=True, \n",
    "    catch_keyboard_interrupt=True, \n",
    "    model_state_file=script_dir + \"/model_storage/cnn/cnn_model.pth\",\n",
    "    embedding_size = 100,\n",
    "    num_channels = 50,\n",
    "    hidden_dim = 200,\n",
    ")\n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    cnn_args.cuda = False\n",
    "    \n",
    "cnn_args.device = torch.device(\"cuda\" if cnn_args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(cnn_args.cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1653781896648,
     "user": {
      "displayName": "Omri Ram",
      "userId": "07837710711281572278"
     },
     "user_tz": -120
    },
    "id": "yw8EEcfzEdR0"
   },
   "outputs": [],
   "source": [
    "dataloader_short_desc = DataLoader(dataset=dataset_short_desc, batch_size=cnn_args.batch_size,\n",
    "                            shuffle=True, drop_last=True)\n",
    "dataloader_long_desc = DataLoader(dataset=dataset_long_desc, batch_size=cnn_args.batch_size,\n",
    "                            shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1653781896648,
     "user": {
      "displayName": "Omri Ram",
      "userId": "07837710711281572278"
     },
     "user_tz": -120
    },
    "id": "oRak4umVEdR0"
   },
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, embedding_size, num_embeddings, num_channels, hidden_dim,\n",
    "     dropout_p, pretrained_embeddings = None, padding_idx = 0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_size (int): size of the embedding vectors\n",
    "            num_embeddings (int): number of embedding vectors\n",
    "            filter_width (int): width of the convolutional kernels\n",
    "            num_channels (int): number of convolutional kernels per layer\n",
    "            hidden_dim (int): the size of the hidden dimension\n",
    "            num_classes (int): the number of classes in classification\n",
    "            dropout_p (float): a dropout parameter \n",
    "            pretrained_embeddings (numpy.array): previously trained word embeddings\n",
    "                default is None. If provided, \n",
    "            padding_idx (int): an index representing a null position\n",
    "        \"\"\"\n",
    "        super(CNNClassifier, self).__init__()\n",
    "\n",
    "        if pretrained_embeddings is None:\n",
    "            self.emb = nn.Embedding(embedding_dim = embedding_size, num_embeddings=num_embeddings, padding_idx=padding_idx)\n",
    "        else:\n",
    "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings)\n",
    "            self.emb = nn.Embedding(embedding_dim=embedding_size,\n",
    "                                    num_embeddings=num_embeddings,\n",
    "                                    padding_idx=padding_idx,\n",
    "                                    _weight=pretrained_embeddings)\n",
    "            \n",
    "        self.convnet = nn.Sequential(nn.Conv1d(in_channels=embedding_size, \n",
    "                   out_channels=num_channels, kernel_size=3),\n",
    "                   nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
    "                   kernel_size=3, stride=1),\n",
    "                  nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels, \n",
    "                   kernel_size=3, stride=2),\n",
    "            nn.ELU())\n",
    "        self._dropout_p = dropout_p\n",
    "        self.fc1 = nn.Linear(num_channels, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, out_features = 1)\n",
    "    def forward(self, x_in, apply_sigmoid=False):\n",
    "        \"\"\"The forward pass of the classifier\n",
    "        \n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor. \n",
    "                x_in.shape should be (batch, dataset._max_seq_length)\n",
    "            apply_sigmoid (bool): a flag for the sigmoid activation\n",
    "                should be false if used with the Cross Entropy losses\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch, num_classes)\n",
    "        \"\"\"\n",
    "        \n",
    "        # embed and permute so features are channels\n",
    "        x_embedded = self.emb(x_in).permute(0, 2, 1)\n",
    "\n",
    "        features = self.convnet(x_embedded)\n",
    "\n",
    "        # average and remove the extra dimension\n",
    "        remaining_size = features.size(dim=2)\n",
    "    \n",
    "        features = F.avg_pool1d(features, remaining_size).squeeze(dim=2)\n",
    "        features = F.dropout(features, p=self._dropout_p)\n",
    "        \n",
    "        # mlp classifier\n",
    "        intermediate_vector = F.relu(F.dropout(self.fc1(features), p=self._dropout_p))\n",
    "        prediction_vector = self.fc2(intermediate_vector).squeeze()\n",
    "\n",
    "        if apply_sigmoid:\n",
    "            prediction_vector = F.sigmoid(prediction_vector)\n",
    "        return prediction_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_kZiStYEdR1"
   },
   "source": [
    "### Training the CNN on the domain specific word2vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsQcSRy2EdR1"
   },
   "source": [
    "In order to explore the effectiveness of the three vectorization methods employed in this assignment, the cnn model will be trained using each of them when deriving the embeddings while using the short description column to train a baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pvr44O7FEdR1"
   },
   "source": [
    "#### Baseline model trained on the short descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460,
     "referenced_widgets": [
      "564c9eee77e442029b0799d0d322b1c6",
      "5f58583b160a46bbbdf1ae854fc01c78",
      "7259f382a97f48ae856489efce683012",
      "72bd3537622b41b9a7bb0eb7bfcda575",
      "69bd3b7874e440a8b84a8eb5aaddf062",
      "d1d9ea4dc3fe41a08757bf2c09e9bc84",
      "69f92914fc164e668ab260006d444196",
      "7c96308ff2234c7ba3824ddae6ba1472",
      "2543bffb12fd4821a588d6f24d504031",
      "cc239e92cd354c5ca7b31bce848e539c",
      "af6215921b93411f85e9790dc325876f",
      "6560923456d04a0c9404fd471ff2e4fa",
      "f57dbc01bf304f8f822cd8acf45891cd",
      "76060baeced3403e993de1dabd25e844",
      "dd098616ac31489fa619ca1ece1e3bac",
      "aeabaee90c5d4ea59a9d1b2c15952c07",
      "e4c1fe3fafcb48949d8a264b8230ab4e",
      "9d606c8cca484d7b89bf5b2ee9d5f896",
      "fa08971a080f40289c23e90c64858efb",
      "543731f3a99d48c2889ebf0c4330b44e",
      "1f86f5806f12426a906ea79344e30759",
      "02882937b8a74b13bfcf33b71a48ea91",
      "c95b34a2562744b7a544792c13fec602",
      "f0cd25d4057b4ff4be563a6e3ca942a8",
      "351f3df2e25547d699c01300806bb119",
      "d5cc831f5ee344a0809aee0349f71b97",
      "9e90dae8f4e441c4bff8034b6d90b267",
      "4ac3037dc9724c24bc4b99f689fe83f6",
      "dd2a13d9a73542618a893cebc8b940f4",
      "2f2f27fdb95a44a4a31b9d37187c0bdc",
      "481a997fa9fd4501842d6e941ace3823",
      "09cd6aeb4cc5431385da434428801067",
      "debbbae35d90474b91844d0e14b11977"
     ]
    },
    "executionInfo": {
     "elapsed": 282209,
     "status": "error",
     "timestamp": 1653782640043,
     "user": {
      "displayName": "Omri Ram",
      "userId": "07837710711281572278"
     },
     "user_tz": -120
    },
    "id": "Eg7LM5fcEdR1",
    "outputId": "b175c59e-0e3f-499b-9b53-8321817c6a57"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564c9eee77e442029b0799d0d322b1c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6560923456d04a0c9404fd471ff2e4fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/415 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95b34a2562744b7a544792c13fec602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-4d356367c452>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mcnn_classifier_short_word2vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_classifier_short_word2vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_short_desc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBCE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mcnn_classifier_short_word2vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-bd878945b3ea>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(classifier, dataset, args, loss_func, BCE, one_hot)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             train_state = update_train_state(args=args, model=classifier,\n\u001b[0;32m--> 108\u001b[0;31m                                           train_state=train_state)\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-db0faf16f30c>\u001b[0m in \u001b[0;36mupdate_train_state\u001b[0;34m(args, model, train_state)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Save one model at least\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrain_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch_index'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_filename'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mtrain_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stop_early'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0m_check_dill_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/model_storage/cnn/cnn_short_word2vec.pth'"
     ]
    }
   ],
   "source": [
    "cnn_args.model_state_file = '/model_storage/cnn/cnn_short_word2vec.pth'\n",
    "words = vectorizer_short_desc.description_vocab._token_to_idx.keys()\n",
    "word_to_idx = {word: idx for idx, word in enumerate(word2vec_model.index2word)}\n",
    "embeddings = make_embedding_matrix(word2vec_model.vectors, word_to_idx, words = words)\n",
    "\n",
    "cnn_classifier_short_word2vec = CNNClassifier(embedding_size=embeddings.shape[1], \n",
    "                            num_embeddings=len(vectorizer_short_desc.description_vocab),\n",
    "                            num_channels=cnn_args.num_channels,\n",
    "                            hidden_dim=cnn_args.hidden_dim,  \n",
    "                            dropout_p=cnn_args.dropout_p,\n",
    "                            pretrained_embeddings=embeddings,\n",
    "                            padding_idx=0)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# train the model \n",
    "cnn_classifier_short_word2vec, train_state = train_model(cnn_classifier_short_word2vec, dataset_short_desc, cnn_args, loss_func, BCE=True)\n",
    "cnn_classifier_short_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 248,
     "status": "aborted",
     "timestamp": 1653782177407,
     "user": {
      "displayName": "Omri Ram",
      "userId": "07837710711281572278"
     },
     "user_tz": -120
    },
    "id": "iSIMS5xxEdR1"
   },
   "outputs": [],
   "source": [
    "test_model(\n",
    "    cnn_classifier_short_word2vec, \n",
    "    dataset= dataset_short_desc, \n",
    "    train_state= train_state, \n",
    "    args = cnn_args,\n",
    "    loss_func=loss_func, \n",
    "    BCE=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kpjAepKEdR2"
   },
   "source": [
    "#### Full description model with word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 248,
     "status": "aborted",
     "timestamp": 1653782177407,
     "user": {
      "displayName": "Omri Ram",
      "userId": "07837710711281572278"
     },
     "user_tz": -120
    },
    "id": "kGFqDHbREdR2"
   },
   "outputs": [],
   "source": [
    "cnn_args.model_state_file = script_dir + '/model_storage/cnn/cnn_long_word2vec.pth'\n",
    "words = vectorizer_long_desc.description_vocab._token_to_idx.keys()\n",
    "word_to_idx = {word: idx for idx, word in enumerate(word2vec_model.index_to_key)}\n",
    "embeddings = make_embedding_matrix(word2vec_model.vectors, word_to_idx, words = words)\n",
    "\n",
    "cnn_classifier_long_word2vec = CNNClassifier(embedding_size=embeddings.shape[1], \n",
    "                            num_embeddings=len(vectorizer_long_desc.description_vocab),\n",
    "                            num_channels=cnn_args.num_channels,\n",
    "                            hidden_dim=cnn_args.hidden_dim,  \n",
    "                            dropout_p=cnn_args.dropout_p,\n",
    "                            pretrained_embeddings=embeddings,\n",
    "                            padding_idx=0)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# train the model \n",
    "cnn_classifier_long_word2vec, train_state = train_model(cnn_classifier_long_word2vec, dataset_long_desc, cnn_args, loss_func, BCE=True)\n",
    "cnn_classifier_long_word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 249,
     "status": "aborted",
     "timestamp": 1653782177408,
     "user": {
      "displayName": "Omri Ram",
      "userId": "07837710711281572278"
     },
     "user_tz": -120
    },
    "id": "7scKurdfEdR2"
   },
   "outputs": [],
   "source": [
    "test_model(\n",
    "    cnn_classifier_long_word2vec, \n",
    "    dataset= dataset_long_desc, \n",
    "    train_state= train_state, \n",
    "    args = cnn_args,\n",
    "    loss_func=loss_func, \n",
    "    BCE=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lc7wt2nfEdR2"
   },
   "source": [
    "### Training the CNN classifier using GLOVE embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bspJwfXEdR3"
   },
   "source": [
    "#### Baseline model with glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1653782177408,
     "user": {
      "displayName": "Omri Ram",
      "userId": "07837710711281572278"
     },
     "user_tz": -120
    },
    "id": "DxtULlg4EdR3"
   },
   "outputs": [],
   "source": [
    "cnn_args.model_state_file = script_dir + '/model_storage/cnn/cnn_short_glove.pth'\n",
    "words = vectorizer_short_desc.description_vocab._token_to_idx.keys()\n",
    "word_to_idx = {word: idx for idx, word in enumerate(glove_model.index_to_key)}\n",
    "embeddings = make_embedding_matrix(glove_model.vectors, word_to_idx, words = words)\n",
    "\n",
    "cnn_classifier_short_glove = CNNClassifier(embedding_size=embeddings.shape[1], \n",
    "                            num_embeddings=len(vectorizer_short_desc.description_vocab),\n",
    "                            num_channels=cnn_args.num_channels,\n",
    "                            hidden_dim=cnn_args.hidden_dim,  \n",
    "                            dropout_p=cnn_args.dropout_p,\n",
    "                            pretrained_embeddings=embeddings,\n",
    "                            padding_idx=0)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# train the model \n",
    "cnn_classifier_short_glove, train_state = train_model(cnn_classifier_short_glove, dataset_short_desc, cnn_args, loss_func, BCE=True)\n",
    "cnn_classifier_short_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1653782177409,
     "user": {
      "displayName": "Omri Ram",
      "userId": "07837710711281572278"
     },
     "user_tz": -120
    },
    "id": "84dgZ2R-EdR3"
   },
   "outputs": [],
   "source": [
    "test_model(\n",
    "    cnn_classifier_short_glove, \n",
    "    dataset= dataset_short_desc, \n",
    "    train_state= train_state, \n",
    "    args = cnn_args,\n",
    "    loss_func=loss_func, \n",
    "    BCE=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFXFJMkQEdR4"
   },
   "source": [
    "#### Full description model with glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1653782177409,
     "user": {
      "displayName": "Omri Ram",
      "userId": "07837710711281572278"
     },
     "user_tz": -120
    },
    "id": "V87WoBfKEdR4"
   },
   "outputs": [],
   "source": [
    "cnn_args.model_state_file = script_dir + '/model_storage/cnn/cnn_long_glove.pth'\n",
    "words = vectorizer_long_desc.description_vocab._token_to_idx.keys()\n",
    "word_to_idx = {word: idx for idx, word in enumerate(glove_model.index_to_key)}\n",
    "embeddings = make_embedding_matrix(glove_model.vectors, word_to_idx, words = words)\n",
    "\n",
    "cnn_classifier_long_glove = CNNClassifier(embedding_size=embeddings.shape[1], \n",
    "                            num_embeddings=len(vectorizer_long_desc.description_vocab),\n",
    "                            num_channels=cnn_args.num_channels,\n",
    "                            hidden_dim=cnn_args.hidden_dim,  \n",
    "                            dropout_p=cnn_args.dropout_p,\n",
    "                            pretrained_embeddings=embeddings,\n",
    "                            padding_idx=0)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "# train the model \n",
    "cnn_classifier_long_glove, train_state = train_model(cnn_classifier_long_glove, dataset_long_desc, cnn_args, loss_func, BCE=True)\n",
    "cnn_classifier_long_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1653782177409,
     "user": {
      "displayName": "Omri Ram",
      "userId": "07837710711281572278"
     },
     "user_tz": -120
    },
    "id": "6YCcovzgEdR4"
   },
   "outputs": [],
   "source": [
    "test_model(\n",
    "    cnn_classifier_long_glove, \n",
    "    dataset= dataset_long_desc, \n",
    "    train_state= train_state, \n",
    "    args = cnn_args,\n",
    "    loss_func=loss_func, \n",
    "    BCE=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_bJmMPCEdR4"
   },
   "source": [
    "### Training the cnn on one hot encoded dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l58pUEJQEdR4"
   },
   "source": [
    "First create the one hot encoding vectorizer and the corresponding one hot embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1653782177410,
     "user": {
      "displayName": "Omri Ram",
      "userId": "07837710711281572278"
     },
     "user_tz": -120
    },
    "id": "eIouIUprEdR5"
   },
   "outputs": [],
   "source": [
    "one_hot = One_hot_Vectorizer.from_dataframe(df, target_cat = False)\n",
    "words = one_hot.description_vocab._token_to_idx.keys()\n",
    "embeddings = make_one_hot_matrix(one_hot, words)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwisXGSOEdR5"
   },
   "source": [
    "#### Baseline one hot encoded classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1653782177410,
     "user": {
      "displayName": "Omri Ram",
      "userId": "07837710711281572278"
     },
     "user_tz": -120
    },
    "id": "wA_PXsxkEdR5"
   },
   "outputs": [],
   "source": [
    "cnn_args.model_state_file = script_dir + '/model_storage/cnn/cnn_short_onehot.pth'\n",
    "cnn_classifier_short_one_hot = CNNClassifier(embedding_size=len(words), \n",
    "                            num_embeddings=len(words),\n",
    "                            num_channels=cnn_args.num_channels,\n",
    "                            hidden_dim=cnn_args.hidden_dim,  \n",
    "                            dropout_p=cnn_args.dropout_p,\n",
    "                            pretrained_embeddings=embeddings,\n",
    "                            padding_idx=0)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# train the model \n",
    "cnn_classifier_short_one_hot, train_state = train_model(cnn_classifier_short_one_hot, dataset_short_desc, cnn_args, loss_func= loss_func, BCE=True)\n",
    "cnn_classifier_short_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1653782177410,
     "user": {
      "displayName": "Omri Ram",
      "userId": "07837710711281572278"
     },
     "user_tz": -120
    },
    "id": "8xP71DlzEdR5"
   },
   "outputs": [],
   "source": [
    "test_model(\n",
    "    cnn_classifier_short_one_hot, \n",
    "    dataset= dataset_short_desc, \n",
    "    train_state= train_state, \n",
    "    args = cnn_args,\n",
    "    loss_func=loss_func, \n",
    "    BCE=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLHkStI_EdR5"
   },
   "source": [
    "#### Classifier on the full description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1653782177411,
     "user": {
      "displayName": "Omri Ram",
      "userId": "07837710711281572278"
     },
     "user_tz": -120
    },
    "id": "2uk3A4JVEdR5"
   },
   "outputs": [],
   "source": [
    "cnn_args.model_state_file = script_dir + '/model_storage/cnn/cnn_long_onehot.pth'\n",
    "cnn_classifier_long_one_hot = CNNClassifier(embedding_size=len(words), \n",
    "                            num_embeddings=len(words),\n",
    "                            num_channels=cnn_args.num_channels,\n",
    "                            hidden_dim=cnn_args.hidden_dim,  \n",
    "                            dropout_p=cnn_args.dropout_p,\n",
    "                            pretrained_embeddings=embeddings,\n",
    "                            padding_idx=0)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# train the model \n",
    "cnn_classifier_long_one_hot, train_state = train_model(cnn_classifier_long_one_hot, dataset_long_desc, cnn_args, loss_func= loss_func, BCE=True)\n",
    "cnn_classifier_long_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "aborted",
     "timestamp": 1653782177411,
     "user": {
      "displayName": "Omri Ram",
      "userId": "07837710711281572278"
     },
     "user_tz": -120
    },
    "id": "oq2EomrrEdR6"
   },
   "outputs": [],
   "source": [
    "test_model(\n",
    "    cnn_classifier_long_one_hot, \n",
    "    dataset= dataset_long_desc, \n",
    "    train_state= train_state, \n",
    "    args = cnn_args,\n",
    "    loss_func=loss_func, \n",
    "    BCE=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "job_type_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "3931f8ce93702ec92af21f632a7247673a82decd980a4ceb2171679cfa65979b"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02882937b8a74b13bfcf33b71a48ea91": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "09cd6aeb4cc5431385da434428801067": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f86f5806f12426a906ea79344e30759": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2543bffb12fd4821a588d6f24d504031": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2f2f27fdb95a44a4a31b9d37187c0bdc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "351f3df2e25547d699c01300806bb119": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2f2f27fdb95a44a4a31b9d37187c0bdc",
      "max": 59,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_481a997fa9fd4501842d6e941ace3823",
      "value": 59
     }
    },
    "481a997fa9fd4501842d6e941ace3823": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4ac3037dc9724c24bc4b99f689fe83f6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "543731f3a99d48c2889ebf0c4330b44e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "564c9eee77e442029b0799d0d322b1c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5f58583b160a46bbbdf1ae854fc01c78",
       "IPY_MODEL_7259f382a97f48ae856489efce683012",
       "IPY_MODEL_72bd3537622b41b9a7bb0eb7bfcda575"
      ],
      "layout": "IPY_MODEL_69bd3b7874e440a8b84a8eb5aaddf062"
     }
    },
    "5f58583b160a46bbbdf1ae854fc01c78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d1d9ea4dc3fe41a08757bf2c09e9bc84",
      "placeholder": "​",
      "style": "IPY_MODEL_69f92914fc164e668ab260006d444196",
      "value": "training routine:   0%"
     }
    },
    "6560923456d04a0c9404fd471ff2e4fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f57dbc01bf304f8f822cd8acf45891cd",
       "IPY_MODEL_76060baeced3403e993de1dabd25e844",
       "IPY_MODEL_dd098616ac31489fa619ca1ece1e3bac"
      ],
      "layout": "IPY_MODEL_aeabaee90c5d4ea59a9d1b2c15952c07"
     }
    },
    "69bd3b7874e440a8b84a8eb5aaddf062": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "69f92914fc164e668ab260006d444196": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7259f382a97f48ae856489efce683012": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7c96308ff2234c7ba3824ddae6ba1472",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2543bffb12fd4821a588d6f24d504031",
      "value": 0
     }
    },
    "72bd3537622b41b9a7bb0eb7bfcda575": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cc239e92cd354c5ca7b31bce848e539c",
      "placeholder": "​",
      "style": "IPY_MODEL_af6215921b93411f85e9790dc325876f",
      "value": " 0/100 [00:00&lt;?, ?it/s]"
     }
    },
    "76060baeced3403e993de1dabd25e844": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fa08971a080f40289c23e90c64858efb",
      "max": 415,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_543731f3a99d48c2889ebf0c4330b44e",
      "value": 415
     }
    },
    "7c96308ff2234c7ba3824ddae6ba1472": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9d606c8cca484d7b89bf5b2ee9d5f896": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9e90dae8f4e441c4bff8034b6d90b267": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aeabaee90c5d4ea59a9d1b2c15952c07": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af6215921b93411f85e9790dc325876f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c95b34a2562744b7a544792c13fec602": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f0cd25d4057b4ff4be563a6e3ca942a8",
       "IPY_MODEL_351f3df2e25547d699c01300806bb119",
       "IPY_MODEL_d5cc831f5ee344a0809aee0349f71b97"
      ],
      "layout": "IPY_MODEL_9e90dae8f4e441c4bff8034b6d90b267"
     }
    },
    "cc239e92cd354c5ca7b31bce848e539c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d1d9ea4dc3fe41a08757bf2c09e9bc84": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5cc831f5ee344a0809aee0349f71b97": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_09cd6aeb4cc5431385da434428801067",
      "placeholder": "​",
      "style": "IPY_MODEL_debbbae35d90474b91844d0e14b11977",
      "value": " 59/59 [04:41&lt;00:00,  4.63it/s, acc=67.7, epoch=0, loss=0.63]"
     }
    },
    "dd098616ac31489fa619ca1ece1e3bac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f86f5806f12426a906ea79344e30759",
      "placeholder": "​",
      "style": "IPY_MODEL_02882937b8a74b13bfcf33b71a48ea91",
      "value": " 415/415 [04:29&lt;00:00,  1.57it/s, acc=67.2, epoch=0, loss=0.639]"
     }
    },
    "dd2a13d9a73542618a893cebc8b940f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "debbbae35d90474b91844d0e14b11977": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e4c1fe3fafcb48949d8a264b8230ab4e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0cd25d4057b4ff4be563a6e3ca942a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ac3037dc9724c24bc4b99f689fe83f6",
      "placeholder": "​",
      "style": "IPY_MODEL_dd2a13d9a73542618a893cebc8b940f4",
      "value": "split=val: 100%"
     }
    },
    "f57dbc01bf304f8f822cd8acf45891cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e4c1fe3fafcb48949d8a264b8230ab4e",
      "placeholder": "​",
      "style": "IPY_MODEL_9d606c8cca484d7b89bf5b2ee9d5f896",
      "value": "split=train: 100%"
     }
    },
    "fa08971a080f40289c23e90c64858efb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
